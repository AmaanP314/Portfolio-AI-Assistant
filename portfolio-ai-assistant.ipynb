{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1dabdf3",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ae8a59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import markdown as md\n",
    "from bs4 import BeautifulSoup\n",
    "import frontmatter\n",
    "from langchain_text_splitters.markdown import MarkdownHeaderTextSplitter\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "aefcf244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about.md',\n",
       " 'amazon-scraper.md',\n",
       " 'audiobook.md',\n",
       " 'com-sent-pros.md',\n",
       " 'curl-counter.md',\n",
       " 'dialexus-chat.md',\n",
       " 'Finetune-RoBERTa-Sentiment.md',\n",
       " 'fraud-detect.md',\n",
       " 'IELTS-Evaluator.md',\n",
       " 'imdb-analysis.md',\n",
       " 'lab-gene-guard.md',\n",
       " 'learning-journey.md',\n",
       " 'music-player.md',\n",
       " 'my-thoughts.md',\n",
       " 'personal-AI-assistant.md',\n",
       " 'recipe-finder.md',\n",
       " 'tweet-scraper.md',\n",
       " 'youtube-AI-Analyzer.md',\n",
       " 'youtube-recommender.md']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_path = os.getcwd()\n",
    "markdown_files = [f for f in os.listdir(directory_path) if f.endswith('.md')]\n",
    "markdown_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "25894175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_markdown(file_path):\n",
    "    post = frontmatter.load(file_path)\n",
    "    meta = post.metadata\n",
    "    body = post.content\n",
    "    return meta, body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dd0b1968",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [(\"#\", \"Header 1\"), (\"##\", \"Header 2\"), (\"###\", \"Header 3\")]\n",
    "header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "chunks = []\n",
    "ids = []\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "for file_path in [markdown_files[5]]:\n",
    "    meta, body = load_markdown(file_path)\n",
    "\n",
    "    split_chunks = header_splitter.split_text(body)\n",
    "\n",
    "    prev_h2 = None\n",
    "    prev_h3_overlap = \"\"\n",
    "    h2_buffer = \"\"  # to hold the ## text until we see if it has ###\n",
    "\n",
    "    for i, c in enumerate(split_chunks):\n",
    "        chunk_id = f\"{meta.get('id')}-{i+1}\"\n",
    "\n",
    "        # Compose header chain\n",
    "        header_context = \"\"\n",
    "        if \"Header 1\" in c.metadata:\n",
    "            header_context += f\"{c.metadata['Header 1']}\\n\"\n",
    "        if \"Header 2\" in c.metadata:\n",
    "            header_context += f\"{c.metadata['Header 2']}\\n\"\n",
    "\n",
    "        # ----------------------------\n",
    "        # Handle ### chunks\n",
    "        # ----------------------------\n",
    "        if \"Header 3\" in c.metadata:\n",
    "            header_text = f\"{c.metadata['Header 3']}\\n\"\n",
    "            # If parent h2 exists, prepend its text\n",
    "            full_text = header_context + h2_buffer + \"\\n\"\n",
    "            # Add overlap if inside same h2\n",
    "            if prev_h2 == c.metadata.get(\"Header 2\") and prev_h3_overlap:\n",
    "                full_text += prev_h3_overlap + \"\\n\"\n",
    "            # Add h3 header and its content\n",
    "            full_text += header_text + c.page_content\n",
    "\n",
    "            # Store last 150 chars of this chunk for overlap\n",
    "            prev_h3_overlap = '...' + c.page_content[-150:]\n",
    "            prev_h2 = c.metadata.get(\"Header 2\")\n",
    "\n",
    "            chunks.append(c)\n",
    "            texts.append(full_text)\n",
    "            metadatas.append({**meta, **c.metadata})\n",
    "            ids.append(chunk_id)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Handle ## chunks\n",
    "        # ----------------------------\n",
    "        elif \"Header 2\" in c.metadata:\n",
    "            # reset h3 overlap\n",
    "            prev_h3_overlap = \"\"\n",
    "            prev_h2 = c.metadata.get(\"Header 2\")\n",
    "\n",
    "            # Save h2 text in case we encounter h3 later\n",
    "            h2_buffer = c.page_content\n",
    "\n",
    "            # Peek ahead: if the next chunk is NOT an h3 under this h2, then this h2 stands alone\n",
    "            next_chunk = split_chunks[i+1] if i+1 < len(split_chunks) else None\n",
    "            if not (next_chunk and \"Header 3\" in next_chunk.metadata and next_chunk.metadata.get(\"Header 2\") == prev_h2):\n",
    "                # It's a standalone ## with no children\n",
    "                full_text = header_context + \"\\n\" + c.page_content\n",
    "                chunks.append(c)\n",
    "                texts.append(full_text)\n",
    "                metadatas.append({**meta, **c.metadata})\n",
    "                ids.append(chunk_id)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Handle # chunks (top-level)\n",
    "        # ----------------------------\n",
    "        elif \"Header 1\" in c.metadata:\n",
    "            prev_h3_overlap = \"\"\n",
    "            prev_h2 = None\n",
    "            h2_buffer = \"\"\n",
    "            full_text = header_context + \"\\n\" + c.page_content\n",
    "            chunks.append(c)\n",
    "            texts.append(full_text)\n",
    "            metadatas.append({**meta, **c.metadata})\n",
    "            ids.append(chunk_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e73fe1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Freelancing: Multi-Tenant Real-Time Chat Application\\n**Overview**\\n\\nThis is a real-time chat application designed for organizations that require complete separation of communication environments. It supports a hierarchical access system — with a global Super Admin overseeing multiple isolated admin-managed tenants. Each admin has full control over their own users, chat groups, and messages, without ever seeing or accessing data from other tenants. The system provides secure authentication, tenant-aware message handling, group messaging, and user management — all with full data isolation, real-time delivery, and an intuitive interface.',\n",
       " 'Freelancing: Multi-Tenant Real-Time Chat Application\\n**Features**\\nThe system follows a strict hierarchical model with well-defined capabilities for each role: **Super Admin**, **Admin**, and **User**. Each tenant operates in complete isolation from others, with a focus on privacy, real-time communication, and role-specific control.\\n**Super Admin (Platform Authority)**\\n- **Authentication**  \\n- Login via secure credential-based access (no self-registration)\\n- Change own password securely  \\n- **Admin Management**  \\n- Create, disable, and soft-delete Admin accounts\\n- Reset or overwrite Admin passwords\\n- View full list of Admins with user/group/message counts  \\n- **Tenant Oversight**  \\n- View all users, groups, and messages across all tenants\\n- Search/filter messages globally (across all tenants)\\n- View tenant-level statistics (e.g., user count, storage, activity)\\n- Monitor system usage and data distribution\\n- Access and audit any tenant’s data  \\n- **Dashboard**  \\n- Summary view of tenants, activity, and system metrics\\n- Optional audit log tracking key actions across the system',\n",
       " 'Freelancing: Multi-Tenant Real-Time Chat Application\\n**Features**\\nThe system follows a strict hierarchical model with well-defined capabilities for each role: **Super Admin**, **Admin**, and **User**. Each tenant operates in complete isolation from others, with a focus on privacy, real-time communication, and role-specific control.\\n... tenant’s data  \\n- **Dashboard**  \\n- Summary view of tenants, activity, and system metrics\\n- Optional audit log tracking key actions across the system\\n**Admin (Tenant Manager)**\\n- **Authentication**  \\n- Login/logout via secure credentials\\n- Change own password  \\n- **User Management**  \\n- Create, delete, and disable users within their tenant\\n- View full user list with basic metadata\\n- Reset user passwords (users cannot self-reset)  \\n- **Group Management**  \\n- Create and delete groups scoped to their tenant\\n- Add or remove users from groups\\n- View and manage all groups and members within the tenant  \\n- **Messaging Oversight**  \\n- Access all private and group chats within the tenant\\n- Filter messages by user, group, or date\\n- Download logs for internal tracking or archiving  \\n- **Chat Access**  \\n- Participate in private and group chats within the tenant\\n- Send and receive messages like a regular user  \\n- **Dashboard**  \\n- View and manage tenant-specific data: users, groups, messages\\n- Track usage trends and internal activity',\n",
       " 'Freelancing: Multi-Tenant Real-Time Chat Application\\n**Features**\\nThe system follows a strict hierarchical model with well-defined capabilities for each role: **Super Admin**, **Admin**, and **User**. Each tenant operates in complete isolation from others, with a focus on privacy, real-time communication, and role-specific control.\\n...ges like a regular user  \\n- **Dashboard**  \\n- View and manage tenant-specific data: users, groups, messages\\n- Track usage trends and internal activity\\n**User (Standard Chat Participant)**\\n- **Authentication**  \\n- Login/logout securely\\n- Password reset handled by Admin only (no self-reset)  \\n- **Private Messaging**  \\n- Chat 1:1 with other users in the same tenant\\n- Access personal chat history\\n- Messages remain private and visible only to the sender and recipient (and the Admin, if needed)  \\n- **Group Messaging**  \\n- Participate in group chats if added by the Admin\\n- View group members and chat history within the group\\n- Upload and view images/files shared in the group  \\n- **Media Support**  \\n- Share images and files in both private and group chats\\n- All files stored securely via Cloudinary with privacy controls  \\n- **Security & Restrictions**  \\n- Cannot view, create, or manage users or groups\\n- Cannot access any other tenant’s data or admin interface',\n",
       " 'Freelancing: Multi-Tenant Real-Time Chat Application\\n**Security**\\n\\nThe application enforces strict role boundaries, tenant isolation, and secure data handling across all components:  \\n- **Tenant Isolation**  \\n- Each tenant’s data (users, messages, groups) is fully isolated; no cross-tenant visibility is possible.  \\n- **Role-Based Access Control (RBAC)**  \\n- Super Admin, Admin, and User roles have clearly scoped permissions.\\n- Access to endpoints and UI elements is strictly filtered by role.  \\n- **Authentication & Sessions**  \\n- Cookie-based JWT authentication with HttpOnly, Secure, and SameSite flags.\\n- All sensitive routes protected by token validation and role checks.\\n- No public registration or open user creation—only Super Admin or Admins can provision accounts.  \\n- **Transport Security**  \\n- All communication over HTTPS (TLS enabled).\\n- Redis pub/sub channels use secure connections (if applicable).  \\n- **Media Access Control**  \\n- Files and images uploaded to Cloudinary using signed URLs.\\n- File URLs are controlled to prevent unauthorized access.  \\n- **Soft Deletion & Message Visibility**  \\n- Messages can be soft-deleted and marked accordingly.\\n- Users can only access their own chat history; Admins may access tenant-level messages for moderation.  \\n- **Rate Limiting & Abuse Protection**  \\n- Messaging and login endpoints can be protected with rate limits (via Redis or middleware layer).\\n- Input sanitization in place to prevent injection attacks.',\n",
       " 'Freelancing: Multi-Tenant Real-Time Chat Application\\nWhy I Built It\\n\\nA client running a lead generation business approached me with a need for a private chat system to manage and monitor internal communication among his employees. The requirements included admin control, group messaging, message tracking, and user management — all under a single secure environment.  \\nSo, Instead of just building a one-off solution, I took it further and developed a multi-tenant system. This allows not only the original client to operate independently but also enables onboarding new clients by simply creating an admin account — no extra setup required. The Super Admin role gives me full control over the system, allowing for centralized oversight, easier maintenance, and the flexibility to scale the platform to serve additional organizations with minimal effort.',\n",
       " 'Freelancing: Multi-Tenant Real-Time Chat Application\\n**How I Built It**\\n\\nStarting from FastAPI for its speed and modular structure, I implemented cookie-based JWT authentication to secure both user and admin sessions. PostgreSQL was used for structured tenant data (users, groups), and MongoDB handled unstructured messages with carefully designed compound indexes for performance. Redis powers both caching and pub/sub to enable real-time updates and system notifications. The frontend was built in Next.js with Tailwind CSS for responsive UI. Docker containers wrapped the entire app for deployment on a self-managed VPS. All critical endpoints enforce strict role checks, and tenant isolation is maintained at the database and logic levels.',\n",
       " 'Freelancing: Multi-Tenant Real-Time Chat Application\\n**Database**\\n\\n- **PostgreSQL:** Stores structured data like users, admins, tenant relationships, groups, and access roles. All records are scoped by admin or tenant ID.\\n- **MongoDB:** Stores messages with a schema that supports both private and group chats. Indexed for sender/receiver, group ID, and timestamps to optimize fetch and scroll performance.\\n- **Redis:** Handles real-time pub/sub channels for message delivery and presence tracking, and also caches frequently accessed data for speed.\\n- **Cloudinary:** External media storage for chat images and files, integrated with secure upload tokens and link expiration for privacy.',\n",
       " 'Freelancing: Multi-Tenant Real-Time Chat Application\\n**Tech Stack**\\n\\n- **Backend:** FastAPI with cookie-based JWT auth, full role and tenant-aware API architecture.\\n- **Frontend:** Next.js and Tailwind CSS, with pages and components scoped to role-based access.\\n- **Databases:** PostgreSQL for relational data; MongoDB for messages with compound indexes.\\n- **Real-Time:** Redis for pub/sub messaging and caching.\\n- **Media:** Cloudinary for secure image/file storage and delivery.\\n- **DevOps:** Dockerized app deployed on VPS with secure environment configuration and HTTPS enforced throughout.',\n",
       " \"Freelancing: Multi-Tenant Real-Time Chat Application\\nLinks:\\n\\nThe GitHub repository for this project is currently private, as it is being used in an active client deployment and contains implementation details specific to their environment.  \\nIf you're interested in using a similar solution for your organization, feel free to reach out to discuss integration or a tailored deployment.\"]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0cd52136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def markdown_to_text(markdown_str: str) -> str:\n",
    "    # 1) Convert MD → HTML\n",
    "    html = md.markdown(markdown_str)\n",
    "    # print(html)\n",
    "    # 2) Strip HTML tags → plain text\n",
    "    return BeautifulSoup(html, \"html.parser\").get_text(separator=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "857f9a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_texts = [markdown_to_text(text) + ' \\n\\n' for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b73cecd",
   "metadata": {},
   "source": [
    "## **Data Embedding** & **Retrieving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "172af62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amaan\\amaan\\AI\\Projects\\personal-ai-assistant\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "from pinecone import Pinecone\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_community.document_compressors import FlashrankRerank\n",
    "from flashrank import Ranker\n",
    "from typing import Optional, List\n",
    "from pydantic import Field\n",
    "from langchain_core.callbacks.manager import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cce4157",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "EMBED_MODEL = os.getenv('EMBEDDING_MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbb5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Vector embedding\n",
    "class FixedDimensionGoogleGenerativeAIEmbeddings(GoogleGenerativeAIEmbeddings):\n",
    "    \"\"\"\n",
    "    A wrapper that fixes the output_dimensionality for embedding methods.\n",
    "    \"\"\"\n",
    "    # Define a Pydantic-compatible field to store the output dimension.\n",
    "    # This makes the field visible to external validation checks.\n",
    "    output_dimensionality: Optional[int] = Field(\n",
    "        None, description=\"The fixed output dimension for embeddings.\"\n",
    "    )\n",
    "\n",
    "    # We override the __init__ to handle the parameter and pass it to the base class.\n",
    "    # The Field definition above will handle the validation, so we don't need a custom pop.\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def embed_documents(self, texts, **kwargs):\n",
    "        if self.output_dimensionality is not None:\n",
    "            kwargs['output_dimensionality'] = self.output_dimensionality\n",
    "        return super().embed_documents(texts, **kwargs)\n",
    "\n",
    "    def embed_query(self, text, **kwargs):\n",
    "        if self.output_dimensionality is not None:\n",
    "            kwargs['output_dimensionality'] = self.output_dimensionality\n",
    "        return super().embed_query(text, **kwargs)\n",
    "\n",
    "# we can pass the output_dimensionality to the constructor directly.\n",
    "embeddings = FixedDimensionGoogleGenerativeAIEmbeddings(\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    model=EMBED_MODEL,\n",
    "    output_dimensionality=768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "655547a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse vector embedding\n",
    "bm25_encoder = BM25Encoder().default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "895a699a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 361.96it/s]\n"
     ]
    }
   ],
   "source": [
    "bm25_encoder.fit(plain_texts)\n",
    "bm25_encoder.dump(\"bm25_values.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d90872c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bm25_encoder = BM25Encoder().load(\"bm25_values.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8d1bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"personal-assistant\"\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbd292d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Retriever configuration\n",
    "namespace = \"portfolio\"\n",
    "# hybrid_retriever = PineconeHybridSearchRetriever(\n",
    "#     embeddings=embeddings,\n",
    "#     sparse_encoder=bm25_encoder,\n",
    "#     index=index,\n",
    "#     top_k=20,\n",
    "#     namespace=namespace  \n",
    "# )\n",
    "\n",
    "class SafeHybridSearchRetriever(PineconeHybridSearchRetriever):\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: Optional[CallbackManagerForRetrieverRun] = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Get documents relevant to the query using hybrid search with fallback to dense-only.\"\"\"\n",
    "        try:\n",
    "            # Try hybrid search first\n",
    "            return super()._get_relevant_documents(query, run_manager=run_manager)\n",
    "        except Exception as e:\n",
    "            # If sparse encoding fails, fall back to dense-only search\n",
    "            if \"Sparse vector must contain at least one value\" in str(e):\n",
    "                print(\"Falling back to dense-only search for query:\", query)\n",
    "                # Generate dense embeddings\n",
    "                embedding = self.embeddings.embed_query(query)\n",
    "                # Search with only dense vectors\n",
    "                results = self.index.query(\n",
    "                    vector=embedding,\n",
    "                    top_k=self.top_k,\n",
    "                    include_metadata=True,\n",
    "                    namespace=self.namespace,\n",
    "                )\n",
    "                # Convert Pinecone results to LangChain documents\n",
    "                return self._process_pinecone_results(results)\n",
    "            else:\n",
    "                # If it's a different error, re-raise it\n",
    "                raise e\n",
    "    \n",
    "    def _process_pinecone_results(self, results):\n",
    "        \"\"\"Process Pinecone results into Document objects.\"\"\"\n",
    "        docs = []\n",
    "        for result in results.matches:\n",
    "            metadata = result.metadata or {}\n",
    "            # Create Document with page content and metadata\n",
    "            doc = Document(\n",
    "                page_content=metadata.pop(\"text\", \"\"),\n",
    "                metadata=metadata,\n",
    "            )\n",
    "            docs.append(doc)\n",
    "        return docs\n",
    "            \n",
    "hybrid_retriever = SafeHybridSearchRetriever( \n",
    "        embeddings=embeddings, \n",
    "        sparse_encoder=bm25_encoder, \n",
    "        index=index,\n",
    "        top_k=20,\n",
    "        namespace=namespace\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14edcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reranker configuration\n",
    "reranker_compressor = FlashrankRerank(\n",
    "    model=\"ms-marco-TinyBERT-L-2-v2\",\n",
    "    top_n=5\n",
    ")\n",
    "\n",
    "retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=reranker_compressor, \n",
    "    base_retriever=hybrid_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "db79078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upserting to Pinecone\n",
    "# hybrid_retriever.add_texts(plain_texts, metadatas=metadatas, ids=ids, namespace=namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dd739352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optionally deleting base on all records or specific prefixes\n",
    "# # 1. Delete ALL records in the index\n",
    "# def delete_all_records():\n",
    "#     index.delete(delete_all=True)\n",
    "#     print(\"✅ All records deleted from index.\")\n",
    "\n",
    "# # 2. Delete records by ID prefix (e.g., 'chat-app')\n",
    "# def delete_records_by_prefix(prefix: str):\n",
    "#     ids_to_delete = [f\"{prefix}-{i}\" for i in range(1, 12)]  # adjust range if needed\n",
    "#     index.delete(ids=ids_to_delete, namespace=\"portfolio\")\n",
    "#     print(f\"✅ Deleted records with prefix '{prefix}-*'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f994656e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 'curl-cnt', 'relevance_score': np.float32(0.0049223006), 'Header 1': 'Curl Counter', 'Header 2': 'Links:', 'duration': 'Jan 2025 - Feb 2025', 'title': 'Curl Counter', 'type': 'project', 'score': 0.131391078}, page_content='Curl Counter\\nLinks: \\n \\n Github Repository: https://github.com/AmaanP314/curl-counter \\n Live App: Unfortunately, there is no live demonstration available for this app at the moment. The memory requirements exceed the limits of the free tier on the hosting platform I use, Render. \\n \\n\\n'),\n",
       " Document(metadata={'id': 'lab-guard', 'relevance_score': np.float32(0.0021973418), 'Header 1': 'Freelancing Work: Lab Gene Guard', 'duration': 'Feb 2025', 'title': 'Lab Gene Guard', 'type': 'freelancing', 'score': 0.136480749}, page_content='Freelancing Work: Lab Gene Guard \\n One of my notable freelancing projects is Lab Gene Guard, a health-tech platform focused on genetic testing. I was responsible for developing and deploying the entire website, which serves as a user-friendly interface for individuals seeking information about genetic tests for various health conditions. \\n\\n'),\n",
       " Document(metadata={'id': 'chat-app', 'relevance_score': np.float32(0.0017538504), 'Header 1': 'Freelancing: Multi-Tenant Real-Time Chat Application', 'Header 2': '**Overview**', 'duration': 'July 2025 - Aug 2025', 'title': 'Multi-tenant Chat Application', 'type': 'freelancing', 'score': 0.129553586}, page_content='Freelancing: Multi-Tenant Real-Time Chat Application\\n Overview \\n This is a real-time chat application designed for organizations that require complete separation of communication environments. It supports a hierarchical access system — with a global Super Admin overseeing multiple isolated admin-managed tenants. Each admin has full control over their own users, chat groups, and messages, without ever seeing or accessing data from other tenants. The system provides secure authentication, tenant-aware message handling, group messaging, and user management — all with full data isolation, real-time delivery, and an intuitive interface. \\n\\n'),\n",
       " Document(metadata={'id': 'lab-guard', 'relevance_score': np.float32(0.0017036701), 'Header 1': 'Freelancing Work: Lab Gene Guard', 'Header 2': 'Conclusion:', 'duration': 'Feb 2025', 'title': 'Lab Gene Guard', 'type': 'freelancing', 'score': 0.132756501}, page_content='Freelancing Work: Lab Gene Guard\\nConclusion: \\n Lab Gene Guard  is a successful freelancing project that involved not just web development but also a deep understanding of SEO practices, responsive design, and creating a user-friendly interface for a niche medical field. It was a great opportunity to work on a health-tech website and contribute to a platform that could potentially improve the lives of users through better access to genetic testing. \\n\\n'),\n",
       " Document(metadata={'id': 'about-me', 'relevance_score': np.float32(0.0015481483), 'Header 1': 'About Amaan Poonawala', 'location': 'Mumbai, India', 'name': 'Amaan Poonawala', 'role': 'Full stack AI Developer', 'type': 'About Me', 'score': 0.288179189}, page_content='About Amaan Poonawala \\n I am an AI and Data Science developer with hands-on experience building intelligent systems and full-stack applications. My work spans large-scale data collection, machine learning model development, and production-ready web apps: \\n- Collected and labeled  1M+ YouTube comments  to fine-tune transformer models (RoBERTa) for sentiment analysis, with the dataset and model downloaded  thousands of times worldwide .\\n- Built a  YouTube AI Analyzer  that integrates search, video and comments summaries and Q&A, comparison visualizations, and comment sentiment insights, combining a full-stack web interface with AI models.\\n- Developed a  Portfolio AI Assistant , an agentic RAG-based chatbot leveraging Pinecone vector DB and rerankers to provide detailed answers about my projects, skills, and learning journey.\\n- Designed and built a  multi-tenant real-time chat platform  for a lead generation business, featuring tenant-level isolation, role-based access (Super Admin, Admin, User), group and private messaging, and centralized monitoring — architected for reuse across multiple organizations.\\n- Experienced in  web scraping , having scraped 20+ real-world websites (Twitter, IMDb, Amazon, Google Maps, LinkedIn, etc.) and processed 1,000+ pages for structured analysis. \\nMy focus is on solving real problems by combining  AI/ML techniques  with  practical engineering  to deliver usable, impactful solutions. \\n\\n')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Querying the retriever\n",
    "result = retriever.invoke(\"has amaan did any freelancing?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe56499",
   "metadata": {},
   "source": [
    "## **LLM Integration**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15ef951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bd683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = os.getenv('LLM_MODEL')\n",
    "llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68e71a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=llm_model,\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    temperature=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6622aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=os.getenv('CUSTOM_PROMPT')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "977b1d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,              \n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": custom_prompt},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6504f5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Portfolio AI Assistant, I am here to answer questions about Amaan Poonawala's professional background.\n"
     ]
    }
   ],
   "source": [
    "query = \"what's your role?\"\n",
    "response = qa_chain.invoke({\"query\": query})\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6943f1",
   "metadata": {},
   "source": [
    "## **Memory Integration**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e1ce655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from typing import Any, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8914b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {} # Simple in-memory store (replace with persistent storage for production)\n",
    "\n",
    "def get_full_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"Retrieves or creates the FULL chat history object for a given session ID.\"\"\"\n",
    "    if session_id not in store:\n",
    "        print(f\"INFO: Creating new chat history for session: {session_id}\")\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    # else:\n",
    "        # print(f\"DEBUG: Accessing existing chat history for session: {session_id}\")\n",
    "    return store[session_id] \n",
    "\n",
    "MAX_HISTORY_TURNS = 3 # Number of recent turns (1 turn = 1 human + 1 AI message)\n",
    "MAX_HISTORY_MESSAGES = MAX_HISTORY_TURNS * 2\n",
    "\n",
    "\n",
    "def limit_history_for_rag_chain(input_dict: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Takes the input dictionary prepared by RunnableWithMessageHistory (potentially\n",
    "    containing full history), trims 'chat_history', and returns the modified dict\n",
    "    for the rag_chain.\n",
    "    \"\"\"\n",
    "    modified_input = input_dict.copy()\n",
    "\n",
    "    if \"chat_history\" in modified_input:\n",
    "        history = modified_input[\"chat_history\"]\n",
    "        if isinstance(history, list) and all(isinstance(m, BaseMessage) for m in history):\n",
    "            limited_history = history[-MAX_HISTORY_MESSAGES:]\n",
    "            modified_input[\"chat_history\"] = limited_history\n",
    "            # print(f\"DEBUG: Passing limited history ({len(limited_history)} msgs) to rag_chain.\")\n",
    "        else:\n",
    "            # This case shouldn't happen with standard history objects, but good to check\n",
    "             print(\"WARN: 'chat_history' in input_dict is not a list of BaseMessages. Passing as is.\")\n",
    "\n",
    "    return modified_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5072a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_prompt_template = os.getenv(\"RETRIEVER_PROMPT\").format(max_turns=MAX_HISTORY_TURNS)\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", retriever_prompt_template),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"), # This will receive the limited history\n",
    "        (\"human\", \"{input}\"),\n",
    "     ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b2f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = os.getenv(\"SYSTEM_PROMPT\").format(max_turns=MAX_HISTORY_TURNS)\n",
    "\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        # MessagesPlaceholder(variable_name=\"chat_history\"), # History is included in the system prompt now\n",
    "        (\"human\", \"{input}\"), # The original user input for the current turn\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6e6f3d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "70eb4bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reformulate_question_chain = (\n",
    "    contextualize_q_prompt # Your existing prompt for this\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = create_stuff_documents_chain(llm, qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d6fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain_that_exposes_question = RunnablePassthrough.assign(\n",
    "    standalone_question=reformulate_question_chain,\n",
    ").assign(\n",
    "    context=lambda x: retriever.invoke(x[\"standalone_question\"]),\n",
    ")\n",
    "\n",
    "rag_chain = retrieval_chain_that_exposes_question.assign(\n",
    "    answer=qa_chain \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f3627719",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    runnable=RunnableLambda(limit_history_for_rag_chain) | rag_chain, # Wrap rag_chain\n",
    "    get_session_history=get_full_session_history, # Use the function returning the ACTUAL history object\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\", # Key used by the wrapper and prompts\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "edeec64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'nice, now tell me about Amaan',\n",
       " 'chat_history': [HumanMessage(content='nice, now tell me about yourself', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I am the Portfolio AI Assistant, an agentic Retrieval-Augmented Generation (RAG) chatbot designed to answer questions about Amaan's professional background.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='how did he manage to build you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The Portfolio AI Assistant was built by Amaan using a combination of hybrid retrieval (dense gemini-embedding-001 + sparse BM25), history-aware query rewriting, semantic, header-aware chunking with contextual overlap, and a dense-only fallback.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='nice, now tell me about Amaan', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I can provide information about Amaan's professional background.\", additional_kwargs={}, response_metadata={})],\n",
       " 'standalone_question': \"What are Amaan's key skills and expertise?\",\n",
       " 'context': [Document(metadata={'id': 'about-me', 'relevance_score': np.float32(0.9972818), 'Header 1': 'About Amaan Poonawala', 'Header 2': 'Professional Snapshot', 'location': 'Mumbai, India', 'name': 'Amaan Poonawala', 'role': 'Full stack AI Developer', 'type': 'About Me', 'score': 0.337192059}, page_content='About Amaan Poonawala\\nProfessional Snapshot \\n \\n Full Name:  Amaan Poonawala \\n Current Education:  3rd year B.Tech Student, Data Science & AI @ Thakur College of Engineering & Technology \\n Current Role:  Full stack AI Developer w/ Data scraping \\n Location:  Mumbai, India \\n Expertise:  Python, JavaScript, Web Scraping, Machine Learning, Data Analysis, Data Science, AI \\n Projects:  Youtube AI Analyzer, Portfolio AI Assistant, Fine-Tune XLM-RoBERTa, 1 Million+ Comment collection & Label, IELTS essay evaluator, Multi-tenant chat application \\n \\n\\n'),\n",
       "  Document(metadata={'id': 'about-me', 'relevance_score': np.float32(0.9488621), 'Header 1': 'About Amaan Poonawala', 'Header 2': 'Key Certifications & Courses', 'location': 'Mumbai, India', 'name': 'Amaan Poonawala', 'role': 'Full stack AI Developer', 'type': 'About Me', 'score': 0.311392903}, page_content='About Amaan Poonawala\\nKey Certifications & Courses \\n \\n The Complete Python Bootcamp: From Zero to Hero in Python (Udemy) \\n Web Scraping in Python: Selenium, Scrapy (Udemy) \\n The Data Analyst Course: Complete Data Analyst Bootcamp (Udemy) \\n The Data Science Course: Complete Data Science Bootcamp 2025 (Udemy) \\n \\n\\n'),\n",
       "  Document(metadata={'id': 'about-me', 'relevance_score': np.float32(0.7873138), 'Header 1': 'About Amaan Poonawala', 'location': 'Mumbai, India', 'name': 'Amaan Poonawala', 'role': 'Full stack AI Developer', 'type': 'About Me', 'score': 0.236572459}, page_content='About Amaan Poonawala \\n I am an AI and Data Science developer with hands-on experience building intelligent systems and full-stack applications. My work spans large-scale data collection, machine learning model development, and production-ready web apps: \\n- Collected and labeled  1M+ YouTube comments  to fine-tune transformer models (RoBERTa) for sentiment analysis, with the dataset and model downloaded  thousands of times worldwide .\\n- Built a  YouTube AI Analyzer  that integrates search, video and comments summaries and Q&A, comparison visualizations, and comment sentiment insights, combining a full-stack web interface with AI models.\\n- Developed a  Portfolio AI Assistant , an agentic RAG-based chatbot leveraging Pinecone vector DB and rerankers to provide detailed answers about my projects, skills, and learning journey.\\n- Designed and built a  multi-tenant real-time chat platform  for a lead generation business, featuring tenant-level isolation, role-based access (Super Admin, Admin, User), group and private messaging, and centralized monitoring — architected for reuse across multiple organizations.\\n- Experienced in  web scraping , having scraped 20+ real-world websites (Twitter, IMDb, Amazon, Google Maps, LinkedIn, etc.) and processed 1,000+ pages for structured analysis. \\nMy focus is on solving real problems by combining  AI/ML techniques  with  practical engineering  to deliver usable, impactful solutions. \\n\\n'),\n",
       "  Document(metadata={'id': 'about-me', 'relevance_score': np.float32(0.019213011), 'Header 1': 'About Amaan Poonawala', 'Header 2': 'Contact & Links', 'location': 'Mumbai, India', 'name': 'Amaan Poonawala', 'role': 'Full stack AI Developer', 'type': 'About Me', 'score': 0.296530306}, page_content='About Amaan Poonawala\\nContact & Links \\n \\n GitHub:  https://github.com/amaanp314 \\n LinkedIn:  https://www.linkedin.com/in/amaan-poonawala \\n Email:  amaanpoonawala05@gmail.com \\n Portfolio  https://amaanp.netlify.app \\n \\n\\n'),\n",
       "  Document(metadata={'id': 'learning-journey', 'relevance_score': np.float32(0.0032779344), 'Header 1': 'Oct 2023 – Dec 2023: Advanced JavaScript & React.js', 'Header 2': 'What I Learned', 'title': 'My Learning Journey', 'type': 'learning', 'score': 0.210576713}, page_content='Oct 2023 – Dec 2023: Advanced JavaScript & React.js\\nWhat I Learned \\n After gaining fundamental knowledge of web development, I decided to deepen my expertise in JavaScript to better understand frameworks like React and Next.js. To achieve this, I completed  The Complete JavaScript Course 2025: From Zero to Expert!  by Jonas Schmedtmann on Udemy. This course helped me learn both fundamental and advanced JavaScript concepts: \\n-  JavaScript Internals:  In-depth understanding of the event loop, call stack, and how JavaScript works behind the scenes.\\n-  Advanced Concepts:  Mastery of closures, first-class functions, and higher-order functions.\\n-  Array Methods:  Practical use of  map ,  reduce ,  filter , and  flatMap  for transforming data.\\n-  Object-Oriented JS:  Class syntax, prototypal inheritance, and design patterns.\\n-  Asynchronous Programming:  Promises, async/await, AJAX, and REST API communication. \\nThis course became a key milestone in strengthening my JavaScript skills and paved the way for my future work in frontend frameworks like React and Next.js. \\n\\n')],\n",
       " 'answer': 'Amaan is a full-stack AI developer with expertise in Python, JavaScript, web scraping, machine learning, data analysis, data science, and AI. He is currently a 3rd-year B.Tech student in Data Science & AI at Thakur College of Engineering & Technology.'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "        {\"input\": \"nice, now tell me about Amaan\"},\n",
    "        config={\"configurable\": {\"session_id\": \"session_id\"}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e2843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatter(session_id: str, query: str):\n",
    "    print(f\"\\n--- Running Query for Session '{session_id}' ---\")\n",
    "    print(f\"User Query: {query}\")\n",
    "\n",
    "    response = conversational_rag_chain.invoke(\n",
    "        {\"input\": query},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "\n",
    "    print(\"\\nReformulated Query for Retrieval:\")\n",
    "    print(f\"{response.get('standalone_question', 'N/A')}\")\n",
    "\n",
    "    print(\"\\nLLM Answer:\")\n",
    "    print(response.get(\"answer\", \"No answer found.\"))\n",
    "\n",
    "    print(\"\\nRetrieved Documents (Context):\")\n",
    "    retrieved_docs = response.get(\"context\", [])\n",
    "    if retrieved_docs:\n",
    "        for i, doc in enumerate(retrieved_docs):\n",
    "            print(f\"  Document {i+1}:\")\n",
    "            print(f\"    Page Content (snippet): {doc.page_content[:200]}...\")\n",
    "            print(f\"    Metadata: {doc.metadata}\")\n",
    "    else:\n",
    "        print(\"  No documents were retrieved.\")\n",
    "\n",
    "    # Print the state of the store to verify saving\n",
    "    print(f\"\\nCurrent Store State for '{session_id}':\")\n",
    "    if session_id in store:\n",
    "         # Access the messages attribute of the ChatMessageHistory object\n",
    "         history_object = store[session_id]\n",
    "         print(f\"  History contains {len(history_object.messages)} messages.\")\n",
    "    else:\n",
    "         print(\"  No history found in store for this session ID.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6060cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df434726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Query for Session '314159' ---\n",
      "User Query: tell me about yourself\n",
      "INFO: Creating new chat history for session: 314159\n",
      "\n",
      "Reformulated Query for Retrieval:\n",
      "What are the capabilities of Portfolio AI Assistant?\n",
      "\n",
      "LLM Answer:\n",
      "I am the Portfolio AI Assistant, an agent designed to answer questions about Amaan Poonawala's professional background. I provide information about his projects, skills, certifications, and learning journey with high precision.\n",
      "\n",
      "Retrieved Documents (Context):\n",
      "  Document 1:\n",
      "    Page Content (snippet): Portfolio AI Assistant\n",
      "Key capabilities (what it actually does well) \n",
      " \n",
      " Context-aware answers  grounded in retrieved chunks (no hallucinated CV claims). \n",
      " Resilient retrieval : hybrid search + dense ...\n",
      "    Metadata: {'id': 'portfolio-ai-assistant', 'relevance_score': np.float32(0.999516), 'Header 1': 'Portfolio AI Assistant', 'Header 2': 'Key capabilities (what it actually does well)', 'duration': 'Apr 2025 – May 2025', 'tags': ['RAG', 'Agentic RAG', 'Pinecone', 'BM25', 'LangChain', 'LLM', 'Hybrid Search'], 'title': 'Portfolio AI Assistant', 'type': 'project', 'score': 0.386481225}\n",
      "  Document 2:\n",
      "    Page Content (snippet): Portfolio AI Assistant \n",
      " The  Portfolio AI Assistant  is an  agentic Retrieval-Augmented Generation (RAG)  chatbot embedded in my portfolio.\n",
      "It answers questions about my  professional background, pro...\n",
      "    Metadata: {'id': 'portfolio-ai-assistant', 'relevance_score': np.float32(0.998919), 'Header 1': 'Portfolio AI Assistant', 'duration': 'Apr 2025 – May 2025', 'tags': ['RAG', 'Agentic RAG', 'Pinecone', 'BM25', 'LangChain', 'LLM', 'Hybrid Search'], 'title': 'Portfolio AI Assistant', 'type': 'project', 'score': 0.45091033}\n",
      "  Document 3:\n",
      "    Page Content (snippet): Portfolio AI Assistant\n",
      "Overview \n",
      " The system combines: \n",
      "-  Hybrid retrieval  (dense  gemini-embedding-001  + sparse  BM25 ),\n",
      "-  History-aware query rewriting  (only when follow-ups need disambiguation...\n",
      "    Metadata: {'id': 'portfolio-ai-assistant', 'relevance_score': np.float32(0.99646854), 'Header 1': 'Portfolio AI Assistant', 'Header 2': 'Overview', 'duration': 'Apr 2025 – May 2025', 'tags': ['RAG', 'Agentic RAG', 'Pinecone', 'BM25', 'LangChain', 'LLM', 'Hybrid Search'], 'title': 'Portfolio AI Assistant', 'type': 'project', 'score': 0.346199095}\n",
      "  Document 4:\n",
      "    Page Content (snippet): Portfolio AI Assistant\n",
      "Links \n",
      " \n",
      " Repo:  https://github.com/AmaanP314/Portfolio-AI-Assistant \n",
      " Live demo:  https://amaanp.netlify.app \n",
      " \n",
      "\n",
      "...\n",
      "    Metadata: {'id': 'portfolio-ai-assistant', 'relevance_score': np.float32(0.99574476), 'Header 1': 'Portfolio AI Assistant', 'Header 2': 'Links', 'duration': 'Apr 2025 – May 2025', 'tags': ['RAG', 'Agentic RAG', 'Pinecone', 'BM25', 'LangChain', 'LLM', 'Hybrid Search'], 'title': 'Portfolio AI Assistant', 'type': 'project', 'score': 0.4115358}\n",
      "  Document 5:\n",
      "    Page Content (snippet): Portfolio AI Assistant\n",
      "Answer synthesis (QA) \n",
      " \n",
      " Uses a compact  stuff-documents  chain with a domain-tuned prompt: \n",
      " Cite  section headers  and  titles  of the supporting chunks. \n",
      " If evidence is wea...\n",
      "    Metadata: {'id': 'portfolio-ai-assistant', 'relevance_score': np.float32(0.9933984), 'Header 1': 'Portfolio AI Assistant', 'Header 2': 'Answer synthesis (QA)', 'duration': 'Apr 2025 – May 2025', 'tags': ['RAG', 'Agentic RAG', 'Pinecone', 'BM25', 'LangChain', 'LLM', 'Hybrid Search'], 'title': 'Portfolio AI Assistant', 'type': 'project', 'score': 0.397579432}\n",
      "\n",
      "Current Store State for '314159':\n",
      "  History contains 2 messages.\n"
     ]
    }
   ],
   "source": [
    "session_1 = \"314159\"\n",
    "\n",
    "# Run multiple queries in the same session\n",
    "formatter(session_1, \"tell me about yourself\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1512d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(query: str, session_id: str):   \n",
    "    response = conversational_rag_chain.invoke(\n",
    "        {\"input\": query},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    return response.get(\"answer\", \"No answer found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
